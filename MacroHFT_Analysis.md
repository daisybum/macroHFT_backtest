# MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading

## ğŸ“‹ ëª©ì°¨
- [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [í•µì‹¬ ì•„í‚¤í…ì²˜](#í•µì‹¬-ì•„í‚¤í…ì²˜)
- [ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ë¶„ì„](#ì•Œê³ ë¦¬ì¦˜-ìƒì„¸-ë¶„ì„)
- [í•™ìŠµ íŒŒì´í”„ë¼ì¸](#í•™ìŠµ-íŒŒì´í”„ë¼ì¸)
- [ì‹¤í–‰ ë°©ë²•](#ì‹¤í–‰-ë°©ë²•)

---

## í”„ë¡œì íŠ¸ ê°œìš”

**MacroHFT**ëŠ” KDD 2024ì— ê²Œì¬ëœ ë…¼ë¬¸ì˜ ê³µì‹ êµ¬í˜„ìœ¼ë¡œ, ê³ ë¹ˆë„ ê±°ë˜(High Frequency Trading)ë¥¼ ìœ„í•œ **ê³„ì¸µì  ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **ê³„ì¸µì  ê°•í™”í•™ìŠµ**: Meta-policyì™€ Sub-policiesì˜ 2ë‹¨ê³„ êµ¬ì¡°
- **ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì‹œì¥ ì¡°ê±´(ì¶”ì„¸, ë³€ë™ì„±)ì— ë”°ë¥¸ ë™ì  ì „ëµ ì„ íƒ
- **ë©”ëª¨ë¦¬ ì¦ê°•**: Episodic Memoryë¥¼ í™œìš©í•œ ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ
- **ëª¨ë°© í•™ìŠµ**: Demonstration Q-tableì„ í†µí•œ ì•ˆì •ì  ì´ˆê¸° í•™ìŠµ

### ë…¼ë¬¸ ì •ë³´
- **ì œëª©**: MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading
- **í•™íšŒ**: KDD 2024
- **arXiv**: https://arxiv.org/abs/2406.14537

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
MacroHFT/
â”œâ”€â”€ env/                          # ê°•í™”í•™ìŠµ í™˜ê²½
â”‚   â”œâ”€â”€ high_level_env.py        # ë©”íƒ€ ì •ì±… í™˜ê²½ (Hyperagent)
â”‚   â””â”€â”€ low_level_env.py         # ì„œë¸Œ ì—ì´ì „íŠ¸ í™˜ê²½
â”‚
â”œâ”€â”€ model/                        # ì‹ ê²½ë§ ëª¨ë¸
â”‚   â””â”€â”€ net.py                   
â”‚       â”œâ”€â”€ subagent             # ì„œë¸Œì—ì´ì „íŠ¸ ë„¤íŠ¸ì›Œí¬ (Dueling DQN + AdaLN)
â”‚       â””â”€â”€ hyperagent           # í•˜ì´í¼ì—ì´ì „íŠ¸ ë„¤íŠ¸ì›Œí¬ (Context-aware)
â”‚
â”œâ”€â”€ RL/                          # ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ low_level.py        # ì„œë¸Œì—ì´ì „íŠ¸ DQN í•™ìŠµ
â”‚   â”‚   â””â”€â”€ high_level.py       # ë©”íƒ€ì •ì±… í•™ìŠµ
â”‚   â””â”€â”€ util/
â”‚       â”œâ”€â”€ replay_buffer.py    # ê²½í—˜ ì¬ìƒ ë²„í¼
â”‚       â”œâ”€â”€ memory.py           # Episodic Memory (í•µì‹¬)
â”‚       â””â”€â”€ utili.py            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚
â”œâ”€â”€ preprocess/                  # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â””â”€â”€ decomposition.py        # ì‹œì¥ ë¶„í•  ë° ë ˆì´ë¸”ë§
â”‚
â”œâ”€â”€ tools/                       # ë„êµ¬
â”‚   â””â”€â”€ demonstration.py        # Q-í…Œì´ë¸” ìƒì„± (ì—­ë°©í–¥ DP)
â”‚
â”œâ”€â”€ data/                        # ë°ì´í„°
â”‚   â”œâ”€â”€ feature_list/           # íŠ¹ì§• ë¦¬ìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ single_features.npy # ìˆœê°„ ê¸°ìˆ  ì§€í‘œ
â”‚   â”‚   â””â”€â”€ trend_features.npy  # ì¶”ì„¸ íŠ¹ì§•
â”‚   â””â”€â”€ ETHUSDT/                # ë°ì´í„°ì…‹ (Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œ)
â”‚       â”œâ”€â”€ train/              # í›ˆë ¨ ë°ì´í„°
â”‚       â”œâ”€â”€ val/                # ê²€ì¦ ë°ì´í„°
â”‚       â”œâ”€â”€ test/               # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚       â””â”€â”€ whole/              # ì „ì²´ ë°ì´í„°
â”‚
â”œâ”€â”€ scripts/                     # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ decomposition.sh        # Step 1: ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ low_level.sh            # Step 2: ì„œë¸Œì—ì´ì „íŠ¸ í•™ìŠµ
â”‚   â””â”€â”€ high_level.sh           # Step 3: ë©”íƒ€ì •ì±… í•™ìŠµ
â”‚
â””â”€â”€ result/                      # í•™ìŠµ ê²°ê³¼
    â”œâ”€â”€ low_level/              # ì„œë¸Œì—ì´ì „íŠ¸ ëª¨ë¸
    â”‚   â””â”€â”€ ETHUSDT/
    â”‚       â””â”€â”€ best_model/
    â”‚           â”œâ”€â”€ slope/      # Slope ì—ì´ì „íŠ¸ 3ê°œ
    â”‚           â””â”€â”€ vol/        # Volatility ì—ì´ì „íŠ¸ 3ê°œ
    â””â”€â”€ high_level/             # ë©”íƒ€ì •ì±… ëª¨ë¸
```

---

## í•µì‹¬ ì•„í‚¤í…ì²˜

### 1. ê³„ì¸µì  êµ¬ì¡° (Hierarchical Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  High-Level: Meta-Policy (Hyperagent)                   â”‚
â”‚  - ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ (slope_360, vol_360)               â”‚
â”‚  - 6ê°œ ì„œë¸Œì—ì´ì „íŠ¸ì˜ ë™ì  ê°€ì¤‘ì¹˜ í• ë‹¹                     â”‚
â”‚  - Episodic Memoryë¡œ ìœ ì‚¬ ê²½í—˜ í™œìš©                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Slope Sub-Agents   â”‚  â”‚ Vol Sub-Agents   â”‚
â”‚ - Agent 1 (label1) â”‚  â”‚ - Agent 1 (label1)â”‚
â”‚ - Agent 2 (label2) â”‚  â”‚ - Agent 2 (label2)â”‚
â”‚ - Agent 3 (label3) â”‚  â”‚ - Agent 3 (label3)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Trading Environment  â”‚
         â”‚  - Gym í™˜ê²½ ê¸°ë°˜       â”‚
         â”‚  - ê±°ë˜ ìˆ˜ìˆ˜ë£Œ í¬í•¨     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ ë¶„í•´ (Market Context Decomposition)

**ëª©ì **: ë³µì¡í•œ ì‹œì¥ì„ ì—¬ëŸ¬ ê°œì˜ ë‹¨ìˆœí•œ ì„œë¸Œ ë§ˆì¼“ìœ¼ë¡œ ë¶„í• 

#### Slope-based Decomposition (ì¶”ì„¸ ê¸°ë°˜)
```python
# 4320 íƒ€ì„ìŠ¤í… ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
1. ê°€ê²© ë°ì´í„°ë¥¼ Butterworth í•„í„°ë¡œ ìŠ¤ë¬´ë”©
2. ì„ í˜• íšŒê·€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
3. Quantile ë¶„í• : [0%, 5%, 35%, 65%, 95%, 100%]
4. ê·¹ë‹¨ê°’ ë³‘í•©: 0â†’1, 4â†’3
5. ìµœì¢…: 3ê°œ ë ˆì´ë¸” (ìƒìŠ¹, ì¤‘ë¦½, í•˜ë½ ì¶”ì„¸)
```

#### Volatility-based Decomposition (ë³€ë™ì„± ê¸°ë°˜)
```python
# ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
1. ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚°
2. Quantile ë¶„í•  ë° ë³‘í•©
3. ìµœì¢…: 3ê°œ ë ˆì´ë¸” (ê³ ë³€ë™, ì¤‘ë³€ë™, ì €ë³€ë™)
```

**ê²°ê³¼**: 3(slope) Ã— 2(vol) = **6ê°œì˜ ì „ë¬¸í™”ëœ ì‹œì¥ ì»¨í…ìŠ¤íŠ¸**

---

## ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ë¶„ì„

### 1. Sub-Agent Architecture (Low-Level Policy)

#### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
```
Input: (single_state, trend_state, previous_action)
â”‚
â”œâ”€> single_state â”€â”€> fc1 â”€â”€> LayerNorm â”€â”€â”
â”‚                                        â”‚
â”œâ”€> trend_state â”€â”€> fc2 â”€â”€â”             â”‚
â”‚                         â”‚             â”‚
â””â”€> previous_action â”€â”€> Embedding â”€â”€â”˜   â”‚
                         â”‚               â”‚
                         â”œâ”€â”€> AdaLN Modulation (shift, scale)
                         â”‚               â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
                                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
    Value Stream          Advantage Stream
        â”‚                       â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”              â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ V(s)  â”‚              â”‚ A(s,a) â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜              â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
```

#### í•µì‹¬ ê¸°ìˆ 

**1) Adaptive Layer Normalization (AdaLN)**
```python
c = action_embedding + trend_state_hidden
shift, scale = AdaLN_modulation(c)
x = LayerNorm(single_state_hidden) * (1 + scale) + shift
```
- ì´ì „ í–‰ë™ê³¼ ì¶”ì„¸ ìƒíƒœë¡œ ì¡°ê±´í™”
- ìƒíƒœ í‘œí˜„ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆ
- Transformerì˜ adaptive normalizationì—ì„œ ì˜ê°

**2) Dueling Architecture**
```python
Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))
```
- Value: ìƒíƒœì˜ ì ˆëŒ€ì  ê°€ì¹˜
- Advantage: í–‰ë™ì˜ ìƒëŒ€ì  ì´ì 
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

### 2. Sub-Agent Training (Low-Level Training)

#### ì†ì‹¤ í•¨ìˆ˜
```python
L_sub = L_TD + Î± Ã— L_KL
```

**TD Loss (Double DQN)**
```python
Q_target = r + Î³ Ã— Q_target(s', argmax_a Q_eval(s', a))
L_TD = MSE(Q_current, Q_target)
```

**KL Divergence Loss (Imitation Learning)**
```python
L_KL = KL(softmax(Q_demo) || softmax(Q_policy))
```
- Q_demo: ì—­ë°©í–¥ ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ê³„ì‚°ëœ ì‹œì—° Qê°’
- ì•ˆì •ì  ì´ˆê¸° í•™ìŠµ ë° ìœ„í—˜ ì œì–´
- Î± ê°’ì´ ë ˆì´ë¸”ë³„ë¡œ ë‹¤ë¦„: 0, 1, 4

#### ì•Œê³ ë¦¬ì¦˜ íë¦„
```
For each epoch:
    For each chunk in training set:
        1. Reset environment with random initial position
        2. For each timestep:
            a. Select action (Îµ-greedy)
            b. Execute action
            c. Store transition in replay buffer
            d. If update condition:
                - Sample batch
                - Compute TD loss
                - Compute KL loss
                - Update eval_net
                - Soft update target_net (Ï„=0.005)
        3. Evaluate on validation set
```

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
- Batch size: 512
- Learning rate: 1e-4
- Gamma: 0.99
- Tau: 0.005
- Epsilon: 0.5 â†’ 0.1 (5 epoch linear decay)
- Update frequency: ë§¤ 100 ìŠ¤í…
- Update times per step: 10íšŒ

### 3. Hyperagent Architecture (High-Level Policy)

#### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
```
Input: (single_state, trend_state, class_state, previous_action)
â”‚
â”œâ”€> concat(single_state, trend_state) â”€â”€> fc1 â”€â”€â”
â”‚                                               â”‚
â”œâ”€> class_state [slope_360, vol_360] â”€â”€> fc2 â”€â”€â”¤
â”‚                                               â”‚
â””â”€> previous_action â”€â”€> Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€> concat([action_hidden, state_hidden])
            â”‚
            â”œâ”€â”€> AdaLN (conditioned on class_state)
            â”‚
            â””â”€â”€> MLP â”€â”€> Softmax â”€â”€> [w1, w2, ..., w6]
                                     (6ê°œ ì„œë¸Œì—ì´ì „íŠ¸ ê°€ì¤‘ì¹˜)
```

#### Q-Value Aggregation (í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜)
```python
# 1. ê° ì„œë¸Œì—ì´ì „íŠ¸ì—ì„œ Qê°’ ê³„ì‚°
Q_slope1 = slope_agent1(s, s_trend, prev_action)  # shape: (batch, 2)
Q_slope2 = slope_agent2(s, s_trend, prev_action)
Q_slope3 = slope_agent3(s, s_trend, prev_action)
Q_vol1 = vol_agent1(s, s_trend, prev_action)
Q_vol2 = vol_agent2(s, s_trend, prev_action)
Q_vol3 = vol_agent3(s, s_trend, prev_action)

# 2. ê°€ì¤‘ì¹˜ ê³„ì‚°
w = hyperagent(s, s_trend, class_state, prev_action)  # shape: (batch, 6)

# 3. ê°€ì¤‘ ì¡°í•©
Q_meta(s, a) = Î£(i=1 to 6) w_i Ã— Q_i(s, a)
```

ìˆ˜ì‹:
$$Q_{meta}(s, a) = \sum_{i=1}^{6} w_i(s) \cdot Q_i^{sub}(s, a)$$

ì—¬ê¸°ì„œ $w_i(s)$ëŠ” ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³€í™”

### 4. Episodic Memory (í•µì‹¬ í˜ì‹ )

#### êµ¬ì¡°
```python
Memory Buffer:
- capacity: 4320 (1 episode)
- K: 5 (nearest neighbors)

Storage: (hidden_state, action, Q_value, single_state, trend_state, prev_action)
```

#### ì•Œê³ ë¦¬ì¦˜

**1) ì €ì¥ (Add)**
```python
def add(h, a, Q, s, s_trend, prev_a):
    buffer[count] = (h, a, Q, s, s_trend, prev_a)
    count = (count + 1) % capacity
```

**2) ê²€ìƒ‰ (Query)**
```python
def query(h_query, a_query):
    # Step 1: ëª¨ë“  ë©”ëª¨ë¦¬ í•­ëª©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    K(h, h_i) = 1 / (||h - h_i||Â² + Îµ)
    
    # Step 2: Top-K ì„ íƒ
    top_k_indices = argsort(K)[-5:]
    
    # Step 3: ê°™ì€ í–‰ë™ë§Œ í•„í„°ë§
    mask = (actions[top_k_indices] == a_query)
    
    # Step 4: ê°€ì¤‘ í‰ê· 
    weights = K[top_k_indices] / Î£ K[top_k_indices]
    masked_weights = weights Ã— mask
    normalized_weights = masked_weights / Î£ masked_weights
    
    Q_memory = Î£(normalized_weights Ã— Q_values[top_k_indices])
    
    return Q_memory
```

ìˆ˜ì‹:
$$Q_{mem}(s, a) = \frac{\sum_{i \in \mathcal{N}_k(s)} K(h, h_i) \cdot Q_i \cdot \mathbb{1}[a_i = a]}{\sum_{i \in \mathcal{N}_k(s)} K(h, h_i) \cdot \mathbb{1}[a_i = a]}$$

**3) ì¬ì¸ì½”ë”© (Re-encode)**
```python
def re_encode(hyperagent):
    # 4320 ìŠ¤í…ë§ˆë‹¤ ì‹¤í–‰
    for batch in memory:
        h_new = hyperagent.encode(s, s_trend, prev_action)
        buffer["hidden_state"] = h_new
```

#### ì—­í• 
- **ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ**: ê³¼ê±° ìœ ì‚¬ ê²½í—˜ ì¬í™œìš©
- **Non-stationarity ëŒ€ì‘**: ì‹œì¥ ë³€í™”ì— ì ì‘
- **ì •ê·œí™”**: Memory Q-valueë¡œ í˜„ì¬ Q-value ë³´ì •

### 5. High-Level Training (Meta-Policy Training)

#### ì†ì‹¤ í•¨ìˆ˜ (3-Term Loss)
```python
L_meta = L_TD + Î± Ã— L_memory + Î² Ã— L_KL
```

**1) TD Loss (Double DQN)**
```python
# Action selection with current eval network
w_next_ = hyperagent(s', s'_trend, class', prev_a')
Q_next_ = Î£(w_next_ Ã— Q_i_sub(s'))
a_argmax = argmax_a Q_next_(s', a)

# Action evaluation with target network
w_next = hyperagent_target(s', s'_trend, class', prev_a')
Q_next = Î£(w_next Ã— Q_i_sub(s'))
Q_target = r + Î³ Ã— (1 - done) Ã— Q_next(s', a_argmax)

# Current Q
w_current = hyperagent(s, s_trend, class, prev_a)
Q_current = Î£(w_current Ã— Q_i_sub(s))

L_TD = MSE(Q_current(s, a), Q_target)
```

**2) Memory Loss (í•µì‹¬)**
```python
# Query episodic memory
h = hyperagent.encode(s, s_trend, prev_a)
Q_memory = episodic_memory.query(h, a)

L_memory = MSE(Q_current, Q_memory)
```
- Î± = 0.5: TD lossì™€ ê· í˜•
- ê³¼ê±° ìœ ì‚¬ ê²½í—˜ìœ¼ë¡œ ì •ê·œí™”

**3) KL Loss (Imitation)**
```python
Q_demo = demonstration_Q_table[t][prev_a][:]
L_KL = KL(softmax(Q_current) || softmax(Q_demo))
```
- Î² = 5: ê°•í•œ ëª¨ë°© í•™ìŠµ

#### ì•Œê³ ë¦¬ì¦˜ íë¦„
```
Initialize:
- Load 6 frozen sub-agents
- Initialize hyperagent and hyperagent_target
- Initialize episodic_memory (capacity=4320)

For each epoch:
    Reset environment
    For each timestep t:
        1. Action selection:
            w = hyperagent(s, s_trend, class, prev_a)
            Q_i = sub_agent_i(s, s_trend, prev_a) for i=1..6
            Q_meta = Î£(w Ã— Q_i)
            a = argmax Q_meta (Îµ-greedy)
        
        2. Execute action:
            s', r, done = env.step(a)
        
        3. Calculate hidden state and Q_memory:
            h = hyperagent.encode(s, s_trend, prev_a)
            Q_next = r + Î³ Ã— Q_estimate(s')
            Q_memory = episodic_memory.query(h, a)
            if Q_memory is NaN:
                Q_memory = Q_next
        
        4. Store in replay buffer:
            store(s, a, r, s', Q_memory)
        
        5. Add to episodic memory:
            episodic_memory.add(h, a, Q_next, s, s_trend, prev_a)
        
        6. Update:
            if t % update_freq == 0:
                for _ in range(update_times):
                    L = L_TD + Î±Ã—L_memory + Î²Ã—L_KL
                    optimize(hyperagent)
                    soft_update(hyperagent_target)
        
        7. Re-encode memory:
            if t % 4320 == 0:
                episodic_memory.re_encode(hyperagent)
    
    Evaluate on validation set
    Save best model
```

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
- Batch size: 512
- Learning rate: 1e-4
- Gamma: 0.99
- Tau: 0.005
- Epsilon: 0.7 â†’ 0.3 (5 epoch linear decay)
- Î± (memory weight): 0.5
- Î² (KL weight): 5
- Update frequency: ë§¤ 512 ìŠ¤í…
- Update times per step: 10íšŒ
- Memory capacity: 4320
- K (nearest neighbors): 5

---

## í•™ìŠµ íŒŒì´í”„ë¼ì¸

### Phase 1: Data Preparation (ë°ì´í„° ì „ì²˜ë¦¬)

```bash
cd MacroHFT
python preprocess/decomposition.py
```

**ì‘ì—… ë‚´ìš©:**
1. ë°ì´í„° ì²­í‚¹ (4320 íƒ€ì„ìŠ¤í… ë‹¨ìœ„)
2. Slope ë ˆì´ë¸”ë§ (3ê°œ í´ë˜ìŠ¤)
3. Volatility ë ˆì´ë¸”ë§ (3ê°œ í´ë˜ìŠ¤)
4. Rolling window íŠ¹ì§• ìƒì„± (slope_360, vol_360)

**ì¶œë ¥:**
```
data/ETHUSDT/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ df_0.feather, df_1.feather, ...
â”‚   â”œâ”€â”€ slope_labels.pkl
â”‚   â””â”€â”€ vol_labels.pkl
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ df_0.feather, df_1.feather, ...
â”‚   â”œâ”€â”€ slope_labels.pkl
â”‚   â””â”€â”€ vol_labels.pkl
â”œâ”€â”€ test/
â”‚   â””â”€â”€ (ë™ì¼ êµ¬ì¡°)
â””â”€â”€ whole/
    â”œâ”€â”€ train.feather
    â”œâ”€â”€ val.feather
    â””â”€â”€ test.feather
```

### Phase 2: Low-Level Training (ì„œë¸Œì—ì´ì „íŠ¸ í•™ìŠµ)

```bash
bash scripts/low_level.sh
```

**6ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ í•™ìŠµ:**

| ì—ì´ì „íŠ¸ | ë¶„ë¥˜ê¸° | ë ˆì´ë¸” | Î± (KL weight) | GPU |
|---------|-------|--------|--------------|-----|
| Slope 1 | slope | 1      | 1            | 0   |
| Slope 2 | slope | 2      | 4            | 1   |
| Slope 3 | slope | 3      | 0            | 2   |
| Vol 1   | vol   | 1      | 4            | 0   |
| Vol 2   | vol   | 2      | 1            | 1   |
| Vol 3   | vol   | 3      | 1            | 2   |

**ê° ì—ì´ì „íŠ¸ í•™ìŠµ ê³¼ì •:**
1. íŠ¹ì • ë ˆì´ë¸”ì˜ ì²­í¬ë“¤ë§Œ ì„ íƒ
2. Demonstration Q-table ìƒì„±
3. DQN + Imitation learningìœ¼ë¡œ í•™ìŠµ
4. ê²€ì¦ ì„±ëŠ¥ ê¸°ë°˜ best model ì €ì¥

**ì¶œë ¥:**
```
result/low_level/ETHUSDT/
â”œâ”€â”€ slope/
â”‚   â”œâ”€â”€ 1/best_model.pkl
â”‚   â”œâ”€â”€ 2/best_model.pkl
â”‚   â””â”€â”€ 3/best_model.pkl
â””â”€â”€ vol/
    â”œâ”€â”€ 1/best_model.pkl
    â”œâ”€â”€ 2/best_model.pkl
    â””â”€â”€ 3/best_model.pkl
```

### Phase 3: High-Level Training (ë©”íƒ€ì •ì±… í•™ìŠµ)

```bash
bash scripts/high_level.sh
```

**í•™ìŠµ ê³¼ì •:**
1. 6ê°œ í•™ìŠµëœ ì„œë¸Œì—ì´ì „íŠ¸ ë¡œë“œ (frozen)
2. Hyperagent ì´ˆê¸°í™”
3. Episodic memory ì´ˆê¸°í™”
4. ì „ì²´ ì—í”¼ì†Œë“œë¡œ í•™ìŠµ:
   - ì„œë¸Œì—ì´ì „íŠ¸ Qê°’ë“¤ì„ ë™ì  ê°€ì¤‘ì¹˜ë¡œ ì¡°í•©
   - Memoryì—ì„œ ìœ ì‚¬ ê²½í—˜ ê²€ìƒ‰
   - 3-term lossë¡œ ì—…ë°ì´íŠ¸
5. ì£¼ê¸°ì  memory re-encoding
6. ê²€ì¦ ì„±ëŠ¥ ê¸°ë°˜ best model ì €ì¥

**ì¶œë ¥:**
```
result/high_level/ETHUSDT/
â”œâ”€â”€ exp1/
â”‚   â””â”€â”€ seed_12345/
â”‚       â”œâ”€â”€ epoch_1/
â”‚       â”‚   â””â”€â”€ trained_model.pkl
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ log/
â””â”€â”€ best_model.pkl
```

---

## ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •

```bash
# Python í™˜ê²½ ìƒì„± (ê¶Œì¥: Python 3.8+)
conda create -n macrohft python=3.8
conda activate macrohft

# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision torchaudio
pip install numpy pandas scipy scikit-learn
pip install gym tensorboard
pip install pyarrow  # feather íŒŒì¼ ì§€ì›
```

### 2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ

Google Driveì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ:
https://drive.google.com/drive/folders/1AYHy-wUV0IwPoA7E1zvMRPL3wK0tPNiY?usp=drive_link

```bash
# ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ë¥¼ data í´ë”ì— ë°°ì¹˜
MacroHFT/
â””â”€â”€ data/
    â””â”€â”€ ETHUSDT/
        â”œâ”€â”€ df_train.feather
        â”œâ”€â”€ df_val.feather
        â””â”€â”€ df_test.feather
```

### 3. ë‹¨ê³„ë³„ ì‹¤í–‰

#### Step 1: ë°ì´í„° ì „ì²˜ë¦¬
```bash
cd MacroHFT
python preprocess/decomposition.py
```

#### Step 2: ì„œë¸Œì—ì´ì „íŠ¸ í•™ìŠµ (ë³‘ë ¬)
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs/low_level/ETHUSDT

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (6ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬)
bash scripts/low_level.sh

# ë˜ëŠ” ê°œë³„ ì‹¤í–‰ ì˜ˆì‹œ
python RL/agent/low_level.py \
    --alpha 1 \
    --clf 'slope' \
    --dataset 'ETHUSDT' \
    --device 'cuda:0' \
    --label 'label_1'
```

#### Step 3: ë©”íƒ€ì •ì±… í•™ìŠµ
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs/high_level

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
bash scripts/high_level.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python RL/agent/high_level.py \
    --dataset 'ETHUSDT' \
    --device 'cuda:0' \
    --alpha 0.5 \
    --beta 5
```

### 4. TensorBoard ëª¨ë‹ˆí„°ë§

```bash
# Low-level í•™ìŠµ ëª¨ë‹ˆí„°ë§
tensorboard --logdir=result/low_level/ETHUSDT/slope/1/seed_12345/log

# High-level í•™ìŠµ ëª¨ë‹ˆí„°ë§
tensorboard --logdir=result/high_level/ETHUSDT/exp1/seed_12345/log
```

**ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­:**
- TD error
- Memory error (high-levelë§Œ)
- KL loss
- Q_eval, Q_target
- Return rate (ìˆ˜ìµë¥ )
- Final balance (ìµœì¢… ì”ì•¡)
- Required money (í•„ìš” ìë³¸)

---

## í•µì‹¬ ê¸°ìˆ  ìš”ì•½

### 1. ê³„ì¸µì  ê°•í™”í•™ìŠµ (Hierarchical RL)
- **Low-level**: íŠ¹ì • ì‹œì¥ ì¡°ê±´ì— íŠ¹í™”ëœ 6ê°œ ì „ë¬¸ê°€ ì •ì±…
- **High-level**: ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì „ë¬¸ê°€ë“¤ì„ ë™ì ìœ¼ë¡œ ì¡°í•©

### 2. ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ (Context-Aware)
- Slopeì™€ Volatilityë¡œ ì‹œì¥ ìƒíƒœ ë¶„ë¥˜
- AdaLNìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë„¤íŠ¸ì›Œí¬ì— ì£¼ì…
- ë™ì  ê°€ì¤‘ì¹˜ë¡œ ìƒí™©ë³„ ìµœì  ì „ëµ ì„ íƒ

### 3. ë©”ëª¨ë¦¬ ì¦ê°• (Memory Augmentation)
- Episodic memoryë¡œ ìœ ì‚¬ ê³¼ê±° ê²½í—˜ ê²€ìƒ‰
- Kernel ê¸°ë°˜ K-NNìœ¼ë¡œ ê´€ë ¨ ê²½í—˜ ê°€ì¤‘ í‰ê· 
- ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ ë° non-stationarity ëŒ€ì‘

### 4. ëª¨ë°© í•™ìŠµ (Imitation Learning)
- Demonstration Q-table (ì—­ë°©í–¥ DP)
- KL divergence lossë¡œ ì•ˆì „í•œ ì •ì±… ìœ ë„
- ì´ˆê¸° í•™ìŠµ ì•ˆì •í™” ë° ìœ„í—˜ ì œì–´

### 5. ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜
- **Dueling DQN**: Valueì™€ Advantage ë¶„ë¦¬
- **AdaLN**: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ì‘ì  ì •ê·œí™”
- **Double DQN**: ê³¼ëŒ€ì¶”ì • ë°©ì§€

---

## ìˆ˜ì‹ ì •ë¦¬

### Low-Level Policy
$$Q^{sub}_i(s, a) = V(s) + A(s, a) - \mathbb{E}_{a'}[A(s, a')]$$

$$\mathcal{L}_{sub} = \underbrace{\mathbb{E}[(Q_{\theta}(s,a) - y)^2]}_{\text{TD Loss}} + \alpha \underbrace{D_{KL}(\text{softmax}(Q_{demo}) \| \text{softmax}(Q_{\theta}))}_{\text{Imitation Loss}}$$

### High-Level Policy
$$Q_{meta}(s, a) = \sum_{i=1}^{6} w_i(s, c) \cdot Q_i^{sub}(s, a)$$

$$\mathcal{L}_{meta} = \underbrace{\mathbb{E}[(Q_{\phi}(s,a) - y)^2]}_{\text{TD Loss}} + \alpha \underbrace{\mathbb{E}[(Q_{\phi}(s,a) - Q_{mem}(s,a))^2]}_{\text{Memory Loss}} + \beta \underbrace{D_{KL}(\text{softmax}(Q_{demo}) \| \text{softmax}(Q_{\phi}))}_{\text{Imitation Loss}}$$

### Episodic Memory
$$Q_{mem}(s, a) = \frac{\sum_{i \in \mathcal{N}_k(s)} K(h_s, h_i) \cdot Q_i \cdot \mathbb{1}[a_i = a]}{\sum_{i \in \mathcal{N}_k(s)} K(h_s, h_i) \cdot \mathbb{1}[a_i = a]}$$

$$K(h, h') = \frac{1}{\|h - h'\|^2 + \epsilon}$$

---

## ì£¼ìš” íŒŒë¼ë¯¸í„° ì •ë¦¬

### ê³µí†µ íŒŒë¼ë¯¸í„°
| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|---|------|
| Batch size | 512 | ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸° |
| Learning rate | 1e-4 | Adam optimizer |
| Gamma (Î³) | 0.99 | í• ì¸ ê³„ìˆ˜ |
| Tau (Ï„) | 0.005 | Soft target update |
| Transaction cost | 0.0002 | ê±°ë˜ ìˆ˜ìˆ˜ë£Œ (0.02%) |
| Max holding | 0.2 (ETH) | ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸° |

### Low-Level íŠ¹í™”
| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|---|------|
| Epsilon | 0.5 â†’ 0.1 | Îµ-greedy decay |
| Decay length | 5 epochs | Epsilon ê°ì†Œ ê¸°ê°„ |
| Update freq | 100 steps | ì—…ë°ì´íŠ¸ ë¹ˆë„ |
| Update times | 10 | ìŠ¤í…ë‹¹ ì—…ë°ì´íŠ¸ íšŸìˆ˜ |
| Î± (KL) | 0, 1, 4 | ë ˆì´ë¸”ë³„ ìƒì´ |

### High-Level íŠ¹í™”
| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|---|------|
| Epsilon | 0.7 â†’ 0.3 | Îµ-greedy decay |
| Decay length | 5 epochs | Epsilon ê°ì†Œ ê¸°ê°„ |
| Update freq | 512 steps | ì—…ë°ì´íŠ¸ ë¹ˆë„ |
| Update times | 10 | ìŠ¤í…ë‹¹ ì—…ë°ì´íŠ¸ íšŸìˆ˜ |
| Î± (memory) | 0.5 | Memory loss ê°€ì¤‘ì¹˜ |
| Î² (KL) | 5 | Imitation loss ê°€ì¤‘ì¹˜ |
| Memory capacity | 4320 | Episodic memory í¬ê¸° |
| K | 5 | K-nearest neighbors |

---

## ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

### ê±°ë˜ ì„±ëŠ¥
- **Return Rate**: `final_balance / required_money`
- **Final Balance**: ìµœì¢… ì”ì•¡ (ìˆ˜ìµ)
- **Required Money**: í•„ìš” ìë³¸ (ìµœëŒ€ ì†ì‹¤ ê¸°ì¤€)
- **Commission Fee**: ì´ ê±°ë˜ ìˆ˜ìˆ˜ë£Œ

### í•™ìŠµ ì„±ëŠ¥
- **TD Error**: Temporal Difference ì˜¤ì°¨
- **Memory Error**: Episodic memory ì˜¤ì°¨
- **KL Loss**: Imitation learning ì†ì‹¤
- **Q Values**: ì˜ˆì¸¡ Qê°’ vs íƒ€ê²Ÿ Qê°’

---

## í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ì—¬

### 1. ê³„ì¸µì  ë¶„í•´ (Hierarchical Decomposition)
ë‹¨ì¼ ë³µì¡í•œ ì •ì±… ëŒ€ì‹  ì—¬ëŸ¬ ì „ë¬¸ê°€ ì •ì±…ìœ¼ë¡œ ë¶„í•´í•˜ì—¬:
- ê° ì„œë¸Œ ì •ì±…ì€ íŠ¹ì • ì‹œì¥ ì¡°ê±´ì— ì§‘ì¤‘
- í•™ìŠµ ë‚œì´ë„ ê°ì†Œ
- í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ

### 2. ë™ì  ì¡°í•© (Dynamic Composition)
ê³ ì •ëœ ì•™ìƒë¸”ì´ ì•„ë‹Œ ìƒí™©ë³„ ë™ì  ê°€ì¤‘ì¹˜:
- ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ìµœì  ì „ëµ ì¡°í•©
- ì ì‘ì„± í–¥ìƒ
- ì¼ë°˜í™” ëŠ¥ë ¥ ê°œì„ 

### 3. ë©”ëª¨ë¦¬ ì¦ê°• (Memory Augmentation)
Episodic memoryë¡œ ê³¼ê±° ê²½í—˜ ì¬í™œìš©:
- ìƒ˜í”Œ íš¨ìœ¨ì„± í¬ê²Œ í–¥ìƒ
- Non-stationary í™˜ê²½ ëŒ€ì‘
- Catastrophic forgetting ë°©ì§€

### 4. ì•ˆì „í•œ íƒìƒ‰ (Safe Exploration)
Demonstrationì„ í†µí•œ ëª¨ë°© í•™ìŠµ:
- ì´ˆê¸° í•™ìŠµ ì•ˆì •í™”
- ìœ„í—˜í•œ í–‰ë™ ì–µì œ
- ìˆ˜ë ´ ì†ë„ í–¥ìƒ

---

## ê²°ë¡ 

MacroHFTëŠ” **ê³„ì¸µì  ê°•í™”í•™ìŠµ**, **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**, **ë©”ëª¨ë¦¬ ì¦ê°•**, **ëª¨ë°© í•™ìŠµ**ì„ ê²°í•©í•˜ì—¬ ê³ ë¹ˆë„ ê±°ë˜ë¼ëŠ” ë³µì¡í•˜ê³  ë¹„ì •ìƒì ì¸ í™˜ê²½ì—ì„œ ê°•ê±´í•˜ê³  ì ì‘ì ì¸ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´
1. **Divide and Conquer**: ë³µì¡í•œ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ë‹¨ìˆœí•œ ë¬¸ì œë¡œ ë¶„í•´
2. **Specialization**: ê° ì„œë¸Œì—ì´ì „íŠ¸ê°€ íŠ¹ì • ì¡°ê±´ì— ì „ë¬¸í™”
3. **Dynamic Adaptation**: ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ê±´ì— ë”°ë¥¸ ë™ì  ì „ëµ ì„ íƒ
4. **Experience Reuse**: ê³¼ê±° ìœ ì‚¬ ê²½í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¬í™œìš©

### ì ìš© ê°€ëŠ¥ì„±
- ê³ ë¹ˆë„ ê±°ë˜ (HFT)
- í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬
- ì‹œì¥ ë©”ì´í‚¹ (Market Making)
- ê¸°íƒ€ non-stationary í™˜ê²½ì˜ ì˜ì‚¬ê²°ì • ë¬¸ì œ

---

## ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: https://arxiv.org/abs/2406.14537
- **GitHub**: (í˜„ì¬ ë””ë ‰í† ë¦¬)
- **ë°ì´í„°**: https://drive.google.com/drive/folders/1AYHy-wUV0IwPoA7E1zvMRPL3wK0tPNiY

---

*ì´ ë¬¸ì„œëŠ” MacroHFT í”„ë¡œì íŠ¸ì˜ ì½”ë“œ ë¶„ì„ì„ í†µí•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ìµœì¢… ì—…ë°ì´íŠ¸: 2025ë…„ 11ì›”*

